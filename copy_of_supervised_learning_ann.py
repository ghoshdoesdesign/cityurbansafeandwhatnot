# -*- coding: utf-8 -*-
"""Copy of Supervised_Learning_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p4KApMQpcyZHp8f625TA1jDdNxXvpM6I
"""

import pandas as pd
from google.colab import files
file = files.upload()
# Change the file name inside "pd.read_csv" as needed
Data_ANN = pd.read_csv("for_training.csv", header=0)

"""Displaying the data."""

Data_ANN

"""Finding the maximum values for each column."""

Data_ANN.max(axis=0)
# axis=1 for each row

"""Finding the minimum for each column."""

Data_ANN.min(axis=0)
# axis=1 for each row

"""## Exploring Data

Let’s first import the graph generating libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns; sns.set(color_codes=True)

"""Let’s plot the distribution of the target variable/label to check if there are significant outliers. Use `plt.subplots()` to change the size of the graph."""

plt.subplots(figsize=(7,7))
# Change the column header inside "DATA_ANN" as needed
sns.distplot(Data_ANN['sky_score']);

"""Next, let's calculate the **correlation matrix** that measures the linear relationships between the variables/features.

The correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation between the two variables. When it is close to -1, the variables have a strong negative correlation.
"""

correlation_matrix = Data_ANN.corr()
pd.set_option('display.max_columns', 20)
pd.set_option('display.width', 2000)
print(correlation_matrix)

corr_df = pd.DataFrame(correlation_matrix )
corr_df.to_csv('corr.csv')

"""Displaying the **correlation matrix graphically**. Use `plt.subplots()` to change the size of the graph.


One variable from couples of variables/lables with high intra-correlation (>= +.90) may be ignored.

Variables/labels with low correlation to the target variable/feature (<= -.70) may be ignored.
"""

correlation_matrix = Data_ANN.corr().round(2)
plt.subplots(figsize=(26,20))
# annot = True to print the values inside the square
sns.heatmap(data=correlation_matrix, annot=True, cmap="coolwarm")

"""Let's plot the variables/lables with high intra-correlation against the target variable/feature. Use `plt.subplots()` to change the size of the graph. Change `'s'` to control the size of the dots."""

#plt.subplots(figsize=(10,10))
sns.regplot(x="rider_score", y="safety_score", data=Data_ANN[['rider_score','safety_score']], scatter_kws={'s':5})

"""We can also generate **paired data distribution** of the all the features and labels. Change `height` to control the size of the output image."""

sns.pairplot(Data_ANN, height=1.5);

"""## Preparing the Data

It is common to separate `y` as the dependent variable/label and `X` as the matrix of independent variables/features.

Here we are using `train_test_split` to split the csv data into test and train sets. It creates 4 subsets: `X_train, X_test, Y_train, Y_test`.

We do this to assess the model’s performance on unseen (test) data.
"""

from sklearn.model_selection  import train_test_split

# y is the dependent variable/label
# Change the column header inside "DATA_ANN" as needed 
Y = Data_ANN['safety_score']

# iloc is used to slice the array into independent variables and dependent variables by index number
# It should be total no of independent variables/features or (dependent variable/label's column number - 1)
# Change the xx values inside [:,0:xx] as needed = total no of independent variables/features or (dependent variable/label's column number - 1)
X = Data_ANN.iloc[:,0:6]

# Split the data into a training set and a test set
# Change test_size value to control the split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=3)

print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

"""## Neural Network Build & Train

First of all, let's define the number of layers and the number of neurons per layer. For supervised learning with numbers, use `RMSprop` as `optimizer` and `mean_squared_error` as `loss`.
"""

from keras.models import Sequential
from keras.layers import Dense

classifier = Sequential() # Initialising the ANN

# Make sure that input_dim = total no of independent variables/features or (dependent variable/label's column number - 1)
classifier.add(Dense(units = 6, activation = 'relu', input_dim = 6))
classifier.add(Dense(units = 4, activation = 'relu'))
classifier.add(Dense(units = 2, activation = 'relu'))
classifier.add(Dense(units = 1, activation = 'relu'))

classifier.compile(optimizer = 'RMSprop', loss = 'mean_squared_error')

"""We will now train the Neural Network using Classifier.fit, passing it the training data, i.e., for this set of X, this is what the Y should look like. Classifier.fit will spot the patterns in the data, and build a Neural Network that could replicate that. """

# increase value of epochs to increase iterations
classifier.fit(X_train, Y_train, batch_size = 1, epochs = 50)

"""## Neural Network Evaluation

Now we will evaluate our model using RMSE, R2-score and accuracy.

We will predict the output (dependent variable/label) using the `X_train` values again, as well as the unseen `X_test` values.
"""

import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# model evaluation for training set
Y_pred_train = classifier.predict(X_train)
# Y_pred_train.where(Y_pred_train >= 0, 0, inplace=True)
# Y_pred_train.where(Y_pred_train <= 1, 1, inplace=True)

rmse = (np.sqrt(mean_squared_error(Y_train, Y_pred_train)))
r2 = r2_score(Y_train, Y_pred_train)
acc = 100 - (rmse/(Y_train.max(axis=0) - Y_train.min(axis=0))*100)

print("The model performance for training set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))
print('Accuracy is {}'.format(acc))
print("\n")

# model evaluation for testing set
Y_pred_test = classifier.predict(X_test)
# Y_pred_test.where(Y_pred_test >= 0, 0, inplace=True)
# Y_pred_test.where(Y_pred_test <= 1, 1, inplace=True)
 
rmse = (np.sqrt(mean_squared_error(Y_test, Y_pred_test)))
r2 = r2_score(Y_test, Y_pred_test)
acc = 100 - (rmse/(Y_test.max(axis=0) - Y_test.min(axis=0))*100)

print("The model performance for testing set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))
print('Accuracy is {}'.format(acc))

"""R2 > .7 = strong ANN, for objective values

R2 > .4 = strong ANN, for subjective values

Let's plot the Predicted vs Real `Y_test` values. If the dots are linear, the Neural Network works well.
"""

plt.subplots(figsize=(10,10))
plt.scatter( Y_test, Y_pred_test, s=50 )
#plt.xlim(0,1)
#plt.ylim(0,1)
plt.xlabel( "Real Y_test")
plt.ylabel( "Predicted Y_test")

"""## Saving Trained Neural Network

Download the zip file named `ANN_model`.
"""

import time
import tensorflow as tf
t = time.time()
export_path_keras = "./ANN_model.h5".format(int(t))
print(export_path_keras)
tf.saved_model.save(classifier, export_path_keras)
!zip -r ANN_model.zip {export_path_keras}

"""## Upload Previously Saved Neural Network

Drag and drop previously saved neural network (named `ANN_model.zip`) in the Files directory on the left.
"""

!unzip ANN_model.zip

import tensorflow
print(tensorflow.__version__)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

"""Load the unzipped neural network files."""

import tensorflow as tf
import tensorflow_hub as hub
upload_path_keras = "./ANN_model.h5"
upload_classifier = tf.keras.models.load_model(
  upload_path_keras,
  custom_objects={'KerasLayer': hub.KerasLayer})
upload_classifier.summary()

"""## Prediction using Neural Network

Let's predict new labels/values based on completely new independent variable/feature values. Firstly, we need to upload the csv file containing the new feature values. Change the file name inside `pd.read_csv` as needed.


"""

import pandas as pd
from google.colab import files
file = files.upload()
# Change the file name inside "pd.read_csv" as needed
X_Predict_ANN = pd.read_csv("testexternal.csv", header=0)

"""Now, let's predict the values and save the predicted values as a csv file. You should see the csv nested in the Files icon on the left."""

# Change "upload_classifier" to "classifier" to use the unsaved trained neural network
# The unsaved trained neural network will not be available for use once the Runtime is over

Y_Predict_ANN = upload_classifier.predict(X_Predict_ANN)
#Y_Predict_ANN.where(Y_Predict_ANN >= 0, 0, inplace=True)
#Y_Predict_ANN.where(Y_Predict_ANN <= 1, 1, inplace=True)
prediction_df = pd.DataFrame(Y_Predict_ANN )
prediction_df.to_csv('prediction_y.csv')

"""## Useful Links

**Graph Plotting Controls**

Heatmap style: https://seaborn.pydata.org/generated/seaborn.heatmap.html

Colour schemes: https://seaborn.pydata.org/generated/seaborn.color_palette.html

Graph style: http://seaborn.pydata.org/tutorial/aesthetics.html

Pairplot style: https://seaborn.pydata.org/generated/seaborn.pairplot.html


**Neural Network Parameters**

Optimizer types: https://keras.io/api/optimizers/

Loss types: https://keras.io/api/losses/
"""