# -*- coding: utf-8 -*-
"""combined ANN final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e1o1jSmJNgzdGL9qY-RsNF3dAeyGEXXl
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive 
drive.mount('/content/drive')
#replace drive path here
# %cd /content/drive/My Drive/Participants_Public/12/Images

import os

input_path = os.getcwd() + '/'

from os import listdir

all_files = [f for f in listdir(input_path)]
### Get only jpg files
jpg_files = list(filter(lambda x: x[-5:] == ('.jpg') or x[-4:] == ('.jpg'), all_files))
jpg_files.sort()

if len(jpg_files) == 0:
  print ('No JPG files found!')

print("JPG Files in the folder:",len(jpg_files))
print (jpg_files)

# Commented out IPython magic to ensure Python compatibility.
#original_code: 'https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb' 

os.mkdir(input_path+'segmentation7')
os.chdir(input_path+'segmentation7')

import pandas as pd


# %tensorflow_version 1.x
import tensorflow as tf
print(tf.__version__)
 
import os
from io import BytesIO
import tarfile
import tempfile
from six.moves import urllib

import matplotlib
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
from PIL import Image
import cv2 as cv
from tqdm import tqdm
import IPython
from sklearn.metrics import confusion_matrix
from tabulate import tabulate
 
# Comment this out if you want to see Deprecation warnings
import warnings
warnings.simplefilter("ignore", DeprecationWarning)

import urllib
import sys
################

class DeepLabModel(object):
    """Class to load deeplab model and run inference."""
 
    FROZEN_GRAPH_NAME = 'frozen_inference_graph'
 
    def __init__(self, tarball_path):
        """Creates and loads pretrained deeplab model."""
        self.graph = tf.Graph()
        graph_def = None
 
        # Extract frozen graph from tar archive.
        tar_file = tarfile.open(tarball_path)
        for tar_info in tar_file.getmembers():
            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):
                file_handle = tar_file.extractfile(tar_info)
                graph_def = tf.GraphDef.FromString(file_handle.read())
                break
        tar_file.close()
 
        if graph_def is None:
            raise RuntimeError('Cannot find inference graph in tar archive.')
 
        with self.graph.as_default():
            tf.import_graph_def(graph_def, name='')
        self.sess = tf.Session(graph=self.graph)
 
    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):
        """Runs inference on a single image.
 
        Args:
            image: A PIL.Image object, raw input image.
            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.
            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.
 
        Returns:
            resized_image: RGB image resized from original input image.
            seg_map: Segmentation map of `resized_image`.
        """
        width, height = image.size
        target_size = (2049,1025)  # size of Cityscapes images
        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)
        batch_seg_map = self.sess.run(
            OUTPUT_TENSOR_NAME,
            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})
        seg_map = batch_seg_map[0]  # expected batch size = 1
        if len(seg_map.shape) == 2:
            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize
        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)
        return seg_map

def create_label_colormap():
    """Creates a label colormap used in Cityscapes segmentation benchmark.
 
    Returns:
        A Colormap for visualizing segmentation results.
    """
    colormap = np.array([
        [128,  64, 128],
        [244,  35, 232],
        [ 70,  70,  70],
        [102, 102, 156],
        [190, 153, 153],
        [153, 153, 153],
        [250, 170,  30],
        [220, 220,   0],
        [107, 142,  35],
        [152, 251, 152],
        [ 70, 130, 180],
        [220,  20,  60],
        [255,   0,   0],
        [  0,   0, 142],
        [  0,   0,  70],
        [  0,  60, 100],
        [  0,  80, 100],
        [  0,   0, 230],
        [119,  11,  32],
        [  0,   0,   0]], dtype=np.uint8)
    return colormap
 
 
def label_to_color_image(label):
    """Adds color defined by the dataset colormap to the label.
 
    Args:
        label: A 2D array with integer type, storing the segmentation label.
 
    Returns:
        result: A 2D array with floating type. The element of the array
            is the color indexed by the corresponding element in the input label
            to the PASCAL color map.
 
    Raises:
        ValueError: If label is not of rank 2 or its value is larger than color
            map maximum entry.
    """
    if label.ndim != 2:
        raise ValueError('Expect 2-D input label')
 
    colormap = create_label_colormap()
 
    if np.max(label) >= len(colormap):
        raise ValueError('label value too large.')
 
    return colormap[label]

################

 
def vis_segmentation(image, seg_map):
    """Visualizes input image, segmentation map and overlay view."""
    plt.figure(figsize=(20, 20))
 
    seg_image = label_to_color_image(seg_map).astype(np.uint8)
    plt.imshow(seg_image)
    plt.axis('off')
    plt.savefig(str(image_id)+'_seg.jpg',bbox_inches='tight')
    plt.close()
 
LABEL_NAMES = np.asarray([
    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',
    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',
    'bus', 'train', 'motorcycle', 'bicycle', 'void'])
 
FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)
FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)

################

MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'
#MODEL_NAME = 'xception65_cityscapes_trainfine'
 
_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'
_MODEL_URLS = {
    'mobilenetv2_coco_cityscapes_trainfine':
        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',
    'xception65_cityscapes_trainfine':
        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',
}
_TARBALL_NAME = 'deeplab_model.tar.gz'
try:
  tk = 'https://drive.google.com/uc?export=download&id=1NKfMlQSrECECSKIwFW_cQha0eNTGglnt'
  tkfile = urllib.request.urlopen(tk)
except:
  sys.exit()
model_dir = tempfile.mkdtemp()
tf.io.gfile.makedirs(model_dir)
 
download_path = os.path.join(model_dir, _TARBALL_NAME)
print('downloading model, this might take a while...')
urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)
print('download completed! loading DeepLab model...')
 
MODEL = DeepLabModel(download_path)
print('model loaded successfully!')

##############

extracted_features = []

for jpg in jpg_files:

  SAMPLE_IMAGE = input_path+jpg
  participant_id = jpg.split('_')[0]
  jpg_id = jpg.split('_')[1]
  image_id = str(participant_id + '_' + jpg_id)
  latitude = float(jpg.split('_')[2])
  longitude = float(jpg.split('_')[3].split('.')[0]+'.'+jpg.split('_')[3].split('.')[1])

  print('running deeplab on '+image_id)
 
  def run_visualization(SAMPLE_IMAGE):
      """Inferences DeepLab model and visualizes result."""
      original_im = Image.open(SAMPLE_IMAGE)
      global seg_map
      seg_map = MODEL.run(original_im)
      vis_segmentation(original_im, seg_map)
 
  run_visualization(SAMPLE_IMAGE)
 
  #print (seg_map)
  total_pixels = seg_map.shape[0]*seg_map.shape[1]
  labels_array = list(np.unique(seg_map, return_counts=True)[0])
  frequency_array = list(np.unique(seg_map, return_counts=True)[1])
  label_indicies = []
  label_frequencies = []
 
  c = 0
  for label in LABEL_NAMES:
    if c in labels_array:
      label_indicies.append(c)
      label_frequencies.append(frequency_array[labels_array.index(c)])
    else:
      label_indicies.append(c)
      label_frequencies.append(0)
    c = c+1
 
  #print(label_indicies)
  #print(label_frequencies)
 
  road_score = (label_frequencies[0]/total_pixels)*100
  sidewalk_score = (label_frequencies[1]/total_pixels)*100
  building_score = (label_frequencies[2]/total_pixels)*100
  wall_score = (label_frequencies[3]/total_pixels)*100
  fence_score = (label_frequencies[4]/total_pixels)*100
  pole_score = (label_frequencies[5]/total_pixels)*100
  traffic_light_score = (label_frequencies[6]/total_pixels)*100
  traffic_sign_score = (label_frequencies[7]/total_pixels)*100
  vegetation_score = (label_frequencies[8]/total_pixels)*100
  terrain_score = (label_frequencies[9]/total_pixels)*100
  sky_score = (label_frequencies[10]/total_pixels)*100
  person_score = (label_frequencies[11]/total_pixels)*100
  rider_score = (label_frequencies[12]/total_pixels)*100
  car_score = (label_frequencies[13]/total_pixels)*100
  truck_score = (label_frequencies[14]/total_pixels)*100
  bus_score = (label_frequencies[15]/total_pixels)*100
  train_score = (label_frequencies[16]/total_pixels)*100
  motorcycle_score = (label_frequencies[17]/total_pixels)*100
  bicycle_score = (label_frequencies[18]/total_pixels)*100
  void_score = (label_frequencies[19]/total_pixels)*100
 
  built_score = building_score + wall_score
  paved_score = road_score + sidewalk_score
  auto_score = car_score + bus_score + truck_score + motorcycle_score
  human_score = person_score + rider_score
  nature_score = terrain_score + vegetation_score
 
 
  extracted_features.append([image_id,built_score,paved_score,auto_score,sky_score,nature_score,human_score,latitude,longitude,road_score,sidewalk_score,building_score,wall_score,fence_score,pole_score,traffic_light_score,traffic_sign_score,vegetation_score,terrain_score,sky_score,person_score,rider_score,car_score,truck_score,bus_score,train_score,motorcycle_score,bicycle_score,void_score])

image_features = pd.DataFrame(extracted_features)

image_features.columns = ['image_id','built_score','paved_score','auto_score','sky_score','nature_score','human_score','latitude', 'longitude','road_score','sidewalk_score','building_score','wall_score','fence_score','pole_score','traffic_light_score','traffic_sign_score','vegetation_score','terrain_score','sky_score','person_score','rider_score','car_score','truck_score','bus_score','train_score','motorcycle_score','bicycle_score','void_score']
image_features = image_features.set_index('image_id')

os.chdir(input_path)

image_features.to_csv(participant_id+'_image_features.csv')

print('Feature Extraction Completed Successfully!')
print('image_features.csv created')

image_features

image_features=image_features.iloc[:,0:6]
image_features

!unzip ANN_model.zip

import tensorflow as tf
import tensorflow_hub as hub
upload_path_keras = "./ANN_model.h5"
upload_classifier = tf.keras.models.load_model(
  upload_path_keras,
  custom_objects={'KerasLayer': hub.KerasLayer})
upload_classifier.summary()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %cd /content/drive/My Drive/Participants_Public/12/Images
X_Predict_ANN = pd.read_csv("12_image_features.csv", header=0)

# Change "upload_classifier" to "classifier" to use the unsaved trained neural network
# The unsaved trained neural network will not be available for use once the Runtime is over

Y_Predict_ANN = upload_classifier.predict(X_Predict_ANN)
#Y_Predict_ANN.where(Y_Predict_ANN >= 0, 0, inplace=True)
#Y_Predict_ANN.where(Y_Predict_ANN <= 1, 1, inplace=True)
prediction_df = pd.DataFrame(Y_Predict_ANN )
prediction_df.to_csv('prediction_y.csv')